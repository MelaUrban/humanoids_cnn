{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1bea3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import color, io\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fcf9057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 36000 files into human database\n",
      "loaded 6000 files into monster database\n",
      "loaded 6000 files into all_humans_annotations database\n",
      "pattern_ii_dataset/humans/dataset/annotations\\female/subject_mesh_0001_anno.json\n",
      "loaded 1000 files into all_monsters_annotations database\n",
      "pattern_ii_dataset/monsters/dataset/annotations\\female/subject_mesh_001_anno.json\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/jdsc-tech-blog/multioutput-cnn-in-pytorch-c5f702d4915f\n",
    "##############################################################\n",
    "# DATA LOADING\n",
    "##############################################################\n",
    "\n",
    "#########################\n",
    "# HUMANS IMAGES\n",
    "#########################\n",
    "\n",
    "\n",
    "humans_path = 'pattern_ii_dataset/humans/dataset/synthetic_images'\n",
    "\n",
    "all_humans = []\n",
    "    \n",
    "for root, dirs, files in sorted(os.walk(humans_path)):\n",
    "    \n",
    "    for name in files:\n",
    "        all_humans.append(str(root) + \"/\" + str(name))\n",
    "        \n",
    "print(\"loaded %d files into human database\" % len(all_humans))\n",
    "\n",
    "#########################\n",
    "# MONSTERS IMAGES\n",
    "#########################\n",
    "\n",
    "monsters_path = 'pattern_ii_dataset/monsters/dataset/synthetic_images'\n",
    "\n",
    "all_monsters = []\n",
    "    \n",
    "for root, dirs, files in sorted(os.walk(monsters_path)):\n",
    "    \n",
    "    for name in files:\n",
    "        all_monsters.append(str(root) + \"/\" + str(name))\n",
    "        \n",
    "print(\"loaded %d files into monster database\" % len(all_monsters))\n",
    "\n",
    "#########################\n",
    "# HUMANS ANNOTATIONS\n",
    "#########################\n",
    "\n",
    "humans_annotations_path = 'pattern_ii_dataset/humans/dataset/annotations'\n",
    "\n",
    "all_humans_annotations = []\n",
    "    \n",
    "for root, dirs, files in sorted(os.walk(humans_annotations_path)):\n",
    "    \n",
    "    for name in files:\n",
    "        all_humans_annotations.append(str(root) + \"/\" + str(name))\n",
    "        \n",
    "print(\"loaded %d files into all_humans_annotations database\" % len(all_humans_annotations))\n",
    "\n",
    "print(all_humans_annotations[0])\n",
    "\n",
    "#########################\n",
    "# MONSTERS ANNOTATIONS\n",
    "#########################\n",
    "\n",
    "monsters_annotations_path = 'pattern_ii_dataset/monsters/dataset/annotations'\n",
    "\n",
    "all_monsters_annotations = []\n",
    "    \n",
    "for root, dirs, files in sorted(os.walk(monsters_annotations_path)):\n",
    "    \n",
    "    for name in files:\n",
    "        all_monsters_annotations.append(str(root) + \"/\" + str(name))\n",
    "        \n",
    "print(\"loaded %d files into all_monsters_annotations database\" % len(all_monsters_annotations))\n",
    "\n",
    "print(all_monsters_annotations[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a3ac931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern_ii_dataset/humans/dataset/synthetic_images\\200x200\\pose0\\female/subject_mesh_w_rgb_0001.png\n",
      "pattern_ii_dataset/monsters/dataset/synthetic_images\\200x200\\pose0\\female/subject_mesh_w_rgb_001.png\n",
      "pattern_ii_dataset/humans/dataset/synthetic_images\\200x200\\pose0\\female/subject_mesh_w_texture_0001.png\n",
      "pattern_ii_dataset/monsters/dataset/synthetic_images\\200x200\\pose0\\female/subject_mesh_w_texture_001.png\n",
      "pattern_ii_dataset/humans/dataset/synthetic_images\\200x200\\pose0\\female/subject_mesh_0001.png\n",
      "pattern_ii_dataset/monsters/dataset/synthetic_images\\200x200\\pose0\\female/subject_mesh_001.png\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# DATA SORTING\n",
    "##############################################################\n",
    "\n",
    "##############################\n",
    "# MONSTERS\n",
    "##############################\n",
    "\n",
    "monsters_plain_paths = []\n",
    "monsters_rgb_paths = []\n",
    "monsters_texture_paths = []\n",
    "\n",
    "for monster in all_monsters:\n",
    "    if \"rgb\" in monster:\n",
    "        monsters_rgb_paths.append(monster)\n",
    "    elif \"texture\" in monster:\n",
    "        monsters_texture_paths.append(monster)\n",
    "    else:\n",
    "        monsters_plain_paths.append(monster)\n",
    "\n",
    "##############################\n",
    "# HUMANS\n",
    "##############################\n",
    "\n",
    "humans_plain_paths = []\n",
    "humans_rgb_paths = []\n",
    "humans_texture_paths = []\n",
    "\n",
    "for human in all_humans:\n",
    "    if \"rgb\" in human:\n",
    "        humans_rgb_paths.append(human)\n",
    "    elif \"texture\" in human:\n",
    "        humans_texture_paths.append(human)\n",
    "    else:\n",
    "        humans_plain_paths.append(human)\n",
    "        \n",
    "##############################\n",
    "# ANNOTATIONS\n",
    "##############################\n",
    "\n",
    "\n",
    "monsters_annotations_df = pd.DataFrame() \n",
    "\n",
    "for monster_annotation_path in all_monsters_annotations:\n",
    "    \n",
    "    with open (monster_annotation_path, 'r') as file:\n",
    "    \n",
    "        # load from json\n",
    "        monster_annotation = json.load(file)\n",
    "        sub_df = pd.DataFrame.from_dict(monster_annotation['human_dimensions'], orient='index')\n",
    "        sub_df = sub_df.transpose()\n",
    "        \n",
    "        monsters_annotations_df = monsters_annotations_df.append(sub_df, ignore_index=True)\n",
    "\n",
    "\n",
    "humans_annotations_df = pd.DataFrame() \n",
    "\n",
    "for humans_annotation_path in all_humans_annotations:\n",
    "    \n",
    "    with open (humans_annotation_path, 'r') as file:\n",
    "    \n",
    "        # load from json\n",
    "        human_annotation = json.load(file)\n",
    "        sub_df = pd.DataFrame.from_dict(human_annotation['human_dimensions'], orient='index')\n",
    "        sub_df = sub_df.transpose()\n",
    "        \n",
    "        humans_annotations_df = humans_annotations_df.append(sub_df, ignore_index=True)\n",
    "    \n",
    "    \n",
    "#print(humans_annotations_df.head())\n",
    "#print(monsters_annotations_df.head())\n",
    "print(humans_rgb_paths[0])    \n",
    "print(monsters_rgb_paths[0])    \n",
    "print(humans_texture_paths[0])    \n",
    "print(monsters_texture_paths[0])    \n",
    "print(humans_plain_paths[0])    \n",
    "print(monsters_plain_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33ca44f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['chest_circumference', 'height', 'inseam', 'left_arm_length',\n",
      "       'pelvis_circumference', 'right_arm_length', 'shoulder_width',\n",
      "       'waist_circumference'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#print(humans_annotations_df.head())\n",
    "print(monsters_annotations_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b31344f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# CONVERT IMAGES TO GRAYSCALE\n",
    "##############################################################\n",
    "\n",
    "# reduce number of images for testing purposes\n",
    "n_images = 1000000\n",
    "\n",
    "humans_plain = []\n",
    "humans_texture_grey = []\n",
    "humans_rgb_grey = []\n",
    "monsters_plain = []\n",
    "monsters_texture_grey = []\n",
    "monsters_rgb_grey = []\n",
    "\n",
    "##############################\n",
    "# HUMANS PLAIN\n",
    "##############################\n",
    "\n",
    "i = 0\n",
    "\n",
    "for file_path in humans_plain_paths:\n",
    "    human_plain = io.imread(file_path)\n",
    "    \n",
    "    # to vector\n",
    "    humans_plain.append(human_plain.flatten())\n",
    "       \n",
    "    if i == n_images:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "\n",
    "##############################\n",
    "# HUMANS W TEXTURE BACKGROUND\n",
    "##############################\n",
    "\n",
    "i = 0\n",
    "\n",
    "for file_path in humans_texture_paths:\n",
    "    human_texture_grey = color.rgb2gray(io.imread(file_path))\n",
    "    \n",
    "    # to vector\n",
    "    human_texture_grey = human_texture_grey.flatten()\n",
    "    \n",
    "    humans_texture_grey.append(human_texture_grey)\n",
    "       \n",
    "    if i == n_images:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "##############################\n",
    "# HUMANS W RGB BACKGROUND\n",
    "##############################\n",
    "\n",
    "i = 0\n",
    "\n",
    "for file_path in humans_rgb_paths:\n",
    "    human_rgb_grey = color.rgb2gray(io.imread(file_path))\n",
    "    \n",
    "    # to vector\n",
    "    human_rgb_grey = human_rgb_grey.flatten()\n",
    "    \n",
    "    humans_rgb_grey.append(human_rgb_grey)\n",
    "       \n",
    "    if i == n_images:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "##############################\n",
    "# MONSTERS PLAIN\n",
    "##############################\n",
    "\n",
    "i = 0\n",
    "\n",
    "for file_path in monsters_plain_paths:\n",
    "    monster_plain = io.imread(file_path)\n",
    "    \n",
    "    # to vector\n",
    "    monsters_plain.append(monster_plain.flatten())\n",
    "       \n",
    "    if i == n_images:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "\n",
    "##############################\n",
    "# MONSTERS W TEXTURE BACKGROUND\n",
    "##############################\n",
    "\n",
    "i = 0\n",
    "\n",
    "for file_path in monsters_texture_paths:\n",
    "    monster_texture_grey = color.rgb2gray(io.imread(file_path))\n",
    "    \n",
    "    # to vector\n",
    "    monster_texture_grey = monster_texture_grey.flatten()\n",
    "    \n",
    "    monsters_texture_grey.append(monster_texture_grey)\n",
    "       \n",
    "    if i == n_images:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "\n",
    "##############################\n",
    "# MONSTERS W RGB BACKGROUND\n",
    "##############################\n",
    "\n",
    "i = 0\n",
    "\n",
    "for file_path in monsters_rgb_paths:\n",
    "    monster_rgb_grey = color.rgb2gray(io.imread(file_path))\n",
    "    \n",
    "    # to vector\n",
    "    monster_rgb_grey = monster_rgb_grey.flatten()\n",
    "    \n",
    "    monsters_rgb_grey.append(monster_rgb_grey)\n",
    "       \n",
    "    if i == n_images:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "\n",
    "    \n",
    "#######################################################################\n",
    "# DISPLAY GREY IMAGES - .flatten() has to be commented inside the loops\n",
    "#######################################################################\n",
    "\n",
    "#cv2.imshow(\"humans_plain\", humans_plain[0])\n",
    "#cv2.imshow(\"humans_rgb_grey\", humans_rgb_grey[0])\n",
    "#cv2.imshow(\"humans_texture_grey\", humans_texture_grey[0])   \n",
    "#cv2.imshow(\"monsters_plain\", monsters_plain[0])     \n",
    "#cv2.imshow(\"monsters_rgb_grey\", monsters_rgb_grey[0])\n",
    "#cv2.imshow(\"monsters_texture_grey\", monsters_texture_grey[5])\n",
    "#cv2.waitKey(0)\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf6a6a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12000, 40008])\n",
      "torch.Size([12000, 40008])\n",
      "torch.Size([12000, 40008])\n",
      "torch.Size([2000, 40008])\n",
      "torch.Size([2000, 40008])\n",
      "torch.Size([2000, 40008])\n"
     ]
    }
   ],
   "source": [
    "# HIER WEITERMACHEN - STATT TENSOREN DATA FRAMES VERWENDEN <----------------------------------------------------\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# CREATE DATA FRAME OF IMAGE VECTORS AND ANNOTATIONS\n",
    "##############################################################\n",
    "\n",
    "##############################\n",
    "# HUMAN PLAIN DATA FRAME\n",
    "##############################\n",
    "\n",
    "\n",
    "# tensor of images w.o. annotations\n",
    "humans_plain_image_tensor = torch.tensor(humans_plain)\n",
    "\n",
    "# tensor of annotations\n",
    "human_annotation_tensor = torch.tensor(humans_annotations_df.values)\n",
    "\n",
    "# copy annotations - pose0 and pose1 have the same annotations\n",
    "human_annotation_tensor = torch.cat((human_annotation_tensor,human_annotation_tensor),0)\n",
    "\n",
    "# tensor of image and annotations\n",
    "humans_plain_full_tensor = torch.cat((humans_plain_image_tensor,human_annotation_tensor),1)\n",
    "\n",
    "\n",
    "##############################\n",
    "# HUMAN RGB DATA FRAME\n",
    "##############################\n",
    "\n",
    "\n",
    "# tensor of images w.o. annotations\n",
    "humans_rgb_image_tensor = torch.tensor(humans_rgb_grey)\n",
    "\n",
    "# tensor of annotations\n",
    "human_annotation_tensor = torch.tensor(humans_annotations_df.values)\n",
    "\n",
    "# copy annotations - pose0 and pose1 have the same annotations\n",
    "human_annotation_tensor = torch.cat((human_annotation_tensor,human_annotation_tensor),0)\n",
    "\n",
    "# tensor of image and annotations\n",
    "humans_rgb_full_tensor = torch.cat((humans_rgb_image_tensor,human_annotation_tensor),1)\n",
    "\n",
    "\n",
    "##############################\n",
    "# HUMAN TEXTURE DATA FRAME\n",
    "##############################\n",
    "\n",
    "\n",
    "# tensor of images w.o. annotations\n",
    "humans_texture_image_tensor = torch.tensor(humans_texture_grey)\n",
    "\n",
    "# tensor of annotations\n",
    "human_annotation_tensor = torch.tensor(humans_annotations_df.values)\n",
    "\n",
    "# copy annotations - pose0 and pose1 have the same annotations\n",
    "human_annotation_tensor = torch.cat((human_annotation_tensor,human_annotation_tensor),0)\n",
    "\n",
    "# tensor of image and annotations\n",
    "humans_texture_full_tensor = torch.cat((humans_texture_image_tensor,human_annotation_tensor),1)\n",
    "\n",
    "\n",
    "##############################\n",
    "# MONSTERS PLAIN DATA FRAME\n",
    "##############################\n",
    "\n",
    "\n",
    "# tensor of images w.o. annotations\n",
    "monsters_plain_image_tensor = torch.tensor(monsters_plain)\n",
    "\n",
    "# tensor of annotations\n",
    "monster_annotation_tensor = torch.tensor(monsters_annotations_df.values)\n",
    "\n",
    "# copy annotations - pose0 and pose1 have the same annotations\n",
    "monster_annotation_tensor = torch.cat((monster_annotation_tensor,monster_annotation_tensor),0)\n",
    "\n",
    "# tensor of image and annotations\n",
    "monsters_plain_full_tensor = torch.cat((monsters_plain_image_tensor,monster_annotation_tensor),1)\n",
    "\n",
    "\n",
    "##############################\n",
    "# MONSTERS RGB DATA FRAME\n",
    "##############################\n",
    "\n",
    "\n",
    "# tensor of images w.o. annotations\n",
    "monster_rgb_image_tensor = torch.tensor(monsters_rgb_grey)\n",
    "\n",
    "# tensor of annotations\n",
    "monster_annotation_tensor = torch.tensor(monsters_annotations_df.values)\n",
    "\n",
    "# copy annotations - pose0 and pose1 have the same annotations\n",
    "monster_annotation_tensor = torch.cat((monster_annotation_tensor,monster_annotation_tensor),0)\n",
    "\n",
    "# tensor of image and annotations\n",
    "monsters_rgb_full_tensor = torch.cat((monster_rgb_image_tensor,monster_annotation_tensor),1)\n",
    "\n",
    "\n",
    "##############################\n",
    "# MONSTERS TEXTURE DATA FRAME\n",
    "##############################\n",
    "\n",
    "\n",
    "# tensor of images w.o. annotations\n",
    "monster_texture_image_tensor = torch.tensor(monsters_texture_grey)\n",
    "\n",
    "# tensor of annotations\n",
    "monster_annotation_tensor = torch.tensor(monsters_annotations_df.values)\n",
    "\n",
    "# copy annotations - pose0 and pose1 have the same annotations\n",
    "monster_annotation_tensor = torch.cat((monster_annotation_tensor,monster_annotation_tensor),0)\n",
    "\n",
    "# tensor of image and annotations\n",
    "monsters_texture_full_tensor = torch.cat((monster_texture_image_tensor,monster_annotation_tensor),1)\n",
    "\n",
    "print(humans_plain_full_tensor.shape)\n",
    "print(humans_rgb_full_tensor.shape)\n",
    "print(humans_texture_full_tensor.shape)\n",
    "print(monsters_plain_full_tensor.shape)\n",
    "print(monsters_rgb_full_tensor.shape)\n",
    "print(monsters_texture_full_tensor.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ec5c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# SAVE FULL TENSORS TO CSV\n",
    "##############################################################\n",
    "\n",
    "##############################\n",
    "# HUMAN AND MONSTERS PLAIN\n",
    "##############################\n",
    "\n",
    "humans_monsters_plain_full_tensor = torch.cat((humans_plain_full_tensor, monsters_plain_full_tensor),0)\n",
    "#hmpf = humans_monsters_plain_full_tensor.numpy()\n",
    "#hmpf_df = pd.DataFrame(hmpf)\n",
    "#hmpf_df.to_csv('humans_monsters_plain.csv')\n",
    "\n",
    "\n",
    "##############################\n",
    "# HUMAN AND MONSTERS RGB\n",
    "##############################\n",
    "\n",
    "humans_monsters_rgb_full_tensor = torch.cat((humans_rgb_full_tensor, monsters_rgb_full_tensor),0)\n",
    "#hmrgbf = humans_monsters_rgb_full_tensor.numpy()\n",
    "#hmrgbf_df = pd.DataFrame(hmrgbf)\n",
    "#hmrgbf_df.to_csv('humans_monsters_rgb.csv')\n",
    "\n",
    "\n",
    "##############################\n",
    "# HUMAN AND MONSTERS TEXTURE\n",
    "##############################\n",
    "\n",
    "humans_monsters_texture_full_tensor = torch.cat((humans_texture_full_tensor, monsters_texture_full_tensor),0)\n",
    "#hmtf = humans_monsters_texture_full_tensor.numpy()\n",
    "#hmtf_df = pd.DataFrame(hmtf)\n",
    "#hmtf_df.to_csv('humans_monsters_texture.csv')\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40f74b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index(['chest_circumference', 'height', 'inseam', 'left_arm_length',\n",
    "#       'pelvis_circumference', 'right_arm_length', 'shoulder_width',\n",
    "#       'waist_circumference'],\n",
    "#      dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e226ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    def __init__(self, dataset, train=True, transform=None):\n",
    "        #Loading csv\n",
    "        data_tensor=dataset\n",
    "        #Leaving only image related  columns\n",
    "        features=data_tensor[:,:39999]\n",
    "        #print(features.shape)\n",
    "        \n",
    "        #Setting labels\n",
    "        label_chest_circumference=data_tensor[:,40000]\n",
    "        label_height=data_tensor[:,40001]\n",
    "        label_inseam=data_tensor[:,40002]\n",
    "        label_left_arm_length=data_tensor[:,40003]\n",
    "        label_pelvis_circumference=data_tensor[:,40004]\n",
    "        label_right_arm_length=data_tensor[:,40005]\n",
    "        label_shoulder_width=data_tensor[:,40006]\n",
    "        label_waist_circumference=data_tensor[:,40007]\n",
    "        \n",
    "        #Splitting the data into train and validation set\n",
    "        X_train, X_test, y_chest_circumference_train, y_chest_circumference_test,\\\n",
    "        y_height_train, y_height_test, y_inseam_train, y_inseam_test,\\\n",
    "        y_left_arm_length_train, y_left_arm_length_test,\\\n",
    "        y_pelvis_circumference_train, y_pelvis_circumference_test,\\\n",
    "        y_right_arm_length_train, y_right_arm_length_test,\\\n",
    "        y_shoulder_width_train, y_shoulder_width_test,\\\n",
    "        y_waist_circumference_train, y_waist_circumference_test = train_test_split(features, label_chest_circumference, \n",
    "                           label_height, label_inseam,\n",
    "                           label_left_arm_length, label_pelvis_circumference,\n",
    "                           label_right_arm_length, label_shoulder_width,\n",
    "                           label_waist_circumference, test_size=0.2)\n",
    "        \n",
    "        if train==True:\n",
    "            self.x=X_train\n",
    "            self.chest_circumference_y=y_chest_circumference_train\n",
    "            self.height_y=y_height_train\n",
    "            self.inseam_y=y_inseam_train\n",
    "            self.left_arm_length_y=y_left_arm_length_train\n",
    "            self.pelvis_circumference_y=y_pelvis_circumference_train\n",
    "            self.right_arm_length_y=y_right_arm_length_train\n",
    "            self.shoulder_width_y=y_shoulder_width_train\n",
    "            self.waist_circumference_y=y_waist_circumference_train\n",
    "        else:\n",
    "            self.x=X_test\n",
    "            self.chest_circumference_y=y_chest_circumference_test\n",
    "            self.height_y=y_height_test\n",
    "            self.inseam_y=y_inseam_test\n",
    "            self.left_arm_length_y=y_left_arm_length_test\n",
    "            self.pelvis_circumference_y=y_pelvis_circumference_test\n",
    "            self.right_arm_length_y=y_right_arm_length_test\n",
    "            self.shoulder_width_y=y_shoulder_width_test\n",
    "            self.waist_circumference_y=y_waist_circumference_test \n",
    "        \n",
    "        #Applying transformation\n",
    "        self.transform=transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image=np.array(self.x[idx, 0:]).astype('float')\n",
    "        label1=np.array([self.chest_circumference_y[idx]]).astype('float')\n",
    "        label2=np.array([self.height_y[idx]]).astype('float')\n",
    "        label3=np.array([self.inseam_y[idx]]).astype('float')\n",
    "        label4=np.array([self.left_arm_length_y[idx]]).astype('float')\n",
    "        label5=np.array([self.pelvis_circumference_y[idx]]).astype('float')\n",
    "        label6=np.array([self.right_arm_length_y[idx]]).astype('float')\n",
    "        label7=np.array([self.shoulder_width_y[idx]]).astype('float')\n",
    "        label8=np.array([self.waist_circumference_y[idx]]).astype('float')\n",
    "        \n",
    "        sample={'image': np.uint8(image), 'label_chest_circumference': label1,\n",
    "                'label_height': label2, 'label_inseam': label3, 'label_left_arm_length': label4,\n",
    "                'label_pelvis_circumference': label5, 'label_.right_arm_length': label6, 'label_shoulder_width': label7,\n",
    "                'label_waist_circumference': label8}\n",
    "        \n",
    "        #Applying transformation\n",
    "        if self.transform:\n",
    "            sample=self.transform(sample)\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c27cedca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#std = humans_monsters_plain_full_tensor\n",
    "\n",
    "#normal = torchvision.transforms.Normalize(mean, std)\n",
    "dataset = MyData(humans_monsters_plain_full_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43230950",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_57092/1620176165.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mbatch_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mmean\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(dataset,\n",
    "                         batch_size=10,\n",
    "                         num_workers=0,\n",
    "                         shuffle=False)\n",
    "\n",
    "mean = 0.0\n",
    "\n",
    "for images in loader:\n",
    "    batch_samples = len(images)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "mean = mean / len(loader.dataset)\n",
    "\n",
    "var = 0.0\n",
    "for images, _ in loader:\n",
    "    batch_samples = len(images)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    var += ((images - mean.unsqueeze(1))**2).sum([0,2])\n",
    "std = torch.sqrt(var / (len(loader.dataset)*224*224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc7b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "  '''\n",
    "    Simple Convolutional Neural Network\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Conv2d(1, 8, kernel_size=5),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(stride=2, kernel_size=1),\n",
    "        \n",
    "      nn.Conv2d(8, 16, kernel_size=5),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(stride=2, kernel_size=1),\n",
    "        \n",
    "      nn.Flatten(),\n",
    "      nn.Linear(),\n",
    "        \n",
    "      nn.ReLU()\n",
    "    )\n",
    "\n",
    "# 20 epochs\n",
    "# mini_batc_size = 100\n",
    "# Loss = MSE\n",
    "# learning_rate = 0.01\n",
    "# momentum = 0.9\n",
    "# TODO: regressor funktion\n",
    "# For layers\n",
    "# \n",
    "# https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html\n",
    "    \n",
    "nn.\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)\n",
    "  \n",
    "    \n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(Net, self).__init__()\n",
    "\n",
    "      # First 2D convolutional layer, taking in 1 input channel (image),\n",
    "      # outputting 32 convolutional features, with a square kernel size of 3\n",
    "      self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "      # Second 2D convolutional layer, taking in the 32 input layers,\n",
    "      # outputting 64 convolutional features, with a square kernel size of 3\n",
    "      self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "\n",
    "      # Designed to ensure that adjacent pixels are either all 0s or all active\n",
    "      # with an input probability\n",
    "      self.dropout1 = nn.Dropout2d(0.25)\n",
    "      self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "      # First fully connected layer\n",
    "      self.fc1 = nn.Linear(9216, 128)\n",
    "      # Second fully connected layer that outputs our 10 labels\n",
    "      self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "my_nn = Net()\n",
    "print(my_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f576d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

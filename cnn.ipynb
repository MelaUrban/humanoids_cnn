{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1bea3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import KFold\n",
    "from skimage import color, io\n",
    "import cv2\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fcf9057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 15992 files into human database\n",
      "loaded 5992 files into monster database\n",
      "loaded 6000 files into all_humans_annotations database\n",
      "pattern_ii_dataset/humans/dataset/annotations\\female/subject_mesh_0001_anno.json\n",
      "loaded 1000 files into all_monsters_annotations database\n",
      "pattern_ii_dataset/monsters/dataset/annotations\\female/subject_mesh_001_anno.json\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/jdsc-tech-blog/multioutput-cnn-in-pytorch-c5f702d4915f\n",
    "##############################################################\n",
    "# DATA LOADING\n",
    "##############################################################\n",
    "\n",
    "#########################\n",
    "# HUMANS IMAGES\n",
    "#########################\n",
    "\n",
    "\n",
    "humans_path = 'pattern_ii_dataset/humans/dataset/synthetic_images'\n",
    "\n",
    "all_humans = []\n",
    "    \n",
    "for root, dirs, files in sorted(os.walk(humans_path)):\n",
    "    \n",
    "    for name in files:\n",
    "        all_humans.append(str(root) + \"/\" + str(name))\n",
    "        \n",
    "print(\"loaded %d files into human database\" % len(all_humans))\n",
    "\n",
    "#########################\n",
    "# MONSTERS IMAGES\n",
    "#########################\n",
    "\n",
    "monsters_path = 'pattern_ii_dataset/monsters/dataset/synthetic_images'\n",
    "\n",
    "all_monsters = []\n",
    "    \n",
    "for root, dirs, files in sorted(os.walk(monsters_path)):\n",
    "    \n",
    "    for name in files:\n",
    "        all_monsters.append(str(root) + \"/\" + str(name))\n",
    "        \n",
    "print(\"loaded %d files into monster database\" % len(all_monsters))\n",
    "\n",
    "#########################\n",
    "# HUMANS ANNOTATIONS\n",
    "#########################\n",
    "\n",
    "humans_annotations_path = 'pattern_ii_dataset/humans/dataset/annotations'\n",
    "\n",
    "all_humans_annotations = []\n",
    "    \n",
    "for root, dirs, files in sorted(os.walk(humans_annotations_path)):\n",
    "    \n",
    "    for name in files:\n",
    "        all_humans_annotations.append(str(root) + \"/\" + str(name))\n",
    "        \n",
    "print(\"loaded %d files into all_humans_annotations database\" % len(all_humans_annotations))\n",
    "\n",
    "print(all_humans_annotations[0])\n",
    "\n",
    "#########################\n",
    "# MONSTERS ANNOTATIONS\n",
    "#########################\n",
    "\n",
    "monsters_annotations_path = 'pattern_ii_dataset/monsters/dataset/annotations'\n",
    "\n",
    "all_monsters_annotations = []\n",
    "    \n",
    "for root, dirs, files in sorted(os.walk(monsters_annotations_path)):\n",
    "    \n",
    "    for name in files:\n",
    "        all_monsters_annotations.append(str(root) + \"/\" + str(name))\n",
    "        \n",
    "print(\"loaded %d files into all_monsters_annotations database\" % len(all_monsters_annotations))\n",
    "\n",
    "print(all_monsters_annotations[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a3ac931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern_ii_dataset/humans/dataset/synthetic_images\\200x200\\pose0\\female/subject_mesh_0001.png\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# DATA SORTING\n",
    "##############################################################\n",
    "\n",
    "##############################\n",
    "# MONSTERS\n",
    "##############################\n",
    "\n",
    "monsters_plain = []\n",
    "monsters_rgb = []\n",
    "monsters_texture = []\n",
    "\n",
    "for monster in all_monsters:\n",
    "    if \"rgb\" in monster:\n",
    "        monsters_rgb.append(monster)\n",
    "    elif \"texture\" in monster:\n",
    "        monsters_texture.append(monster)\n",
    "    else:\n",
    "        monsters_plain.append(monster)\n",
    "\n",
    "##############################\n",
    "# HUMANS\n",
    "##############################\n",
    "\n",
    "humans_plain = []\n",
    "humans_rgb = []\n",
    "humans_texture = []\n",
    "\n",
    "for human in all_humans:\n",
    "    if \"rgb\" in human:\n",
    "        humans_rgb.append(human)\n",
    "    elif \"texture\" in human:\n",
    "        humans_texture.append(human)\n",
    "    else:\n",
    "        humans_plain.append(human)\n",
    "        \n",
    "##############################\n",
    "# ANNOTATIONS\n",
    "##############################\n",
    "\n",
    "\n",
    "monsters_annotations_df = pd.DataFrame() \n",
    "\n",
    "for monster_annotation_path in all_monsters_annotations:\n",
    "    \n",
    "    with open (monster_annotation_path, 'r') as file:\n",
    "    \n",
    "        # load from json\n",
    "        monster_annotation = json.load(file)\n",
    "        sub_df = pd.DataFrame.from_dict(monster_annotation['human_dimensions'], orient='index')\n",
    "        sub_df = sub_df.transpose()\n",
    "        \n",
    "        monsters_annotations_df = monsters_annotations_df.append(sub_df, ignore_index=True)\n",
    "\n",
    "\n",
    "humans_annotations_df = pd.DataFrame() \n",
    "\n",
    "for humans_annotation_path in all_humans_annotations:\n",
    "    \n",
    "    with open (humans_annotation_path, 'r') as file:\n",
    "    \n",
    "        # load from json\n",
    "        human_annotation = json.load(file)\n",
    "        sub_df = pd.DataFrame.from_dict(human_annotation['human_dimensions'], orient='index')\n",
    "        sub_df = sub_df.transpose()\n",
    "        \n",
    "        humans_annotations_df = humans_annotations_df.append(sub_df, ignore_index=True)\n",
    "    \n",
    "    \n",
    "#print(humans_annotations_df.head())\n",
    "#print(monsters_annotations_df.head())\n",
    "#print(humans_rgb[0])    \n",
    "#print(monsters_rgb[0])    \n",
    "#print(humans_texture[0])    \n",
    "#print(monsters_texture[0])    \n",
    "print(humans_plain[0])    \n",
    "#print(monsters_plain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ca44f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(humans_annotations_df.head())\n",
    "print(monsters_annotations_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b31344f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-0f8c133328b1>:4: FutureWarning: The behavior of rgb2gray will change in scikit-image 0.19. Currently, rgb2gray allows 2D grayscale image to be passed as inputs and leaves them unmodified as outputs. Starting from version 0.19, 2D arrays will be treated as 1D images with 3 channels.\n",
      "  human_image = color.rgb2gray(io.imread(file))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "human_image_list = []\n",
    "\n",
    "for file in humans_plain:\n",
    "    human_image = color.rgb2gray(io.imread(file))\n",
    "    # to vector\n",
    "    human_image = human_image.flatten()\n",
    "    \n",
    "    human_image_list.append(human_image)\n",
    "    \n",
    "print(\"finished\")\n",
    "\n",
    "human_image_tensor = torch.tensor(human_image_list)\n",
    "human_annotation_tensor = torch.tensor(humans_annotations_df.values)\n",
    "human_annotation_tensor = torch.cat((human_annotation_tensor,human_annotation_tensor),0)\n",
    "\n",
    "\n",
    "humans_tensor = torch.cat((human_image_tensor,human_annotation_tensor),1)\n",
    "\n",
    "print(\"finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc7b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "  '''\n",
    "    Simple Convolutional Neural Network\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Conv2d(1, 8, kernel_size=5),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(stride=2, kernel_size=2),\n",
    "        \n",
    "      nn.Conv2d(8, 16, kernel_size=5),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(stride=2, kernel_size=2),\n",
    "        \n",
    "      nn.Flatten(),\n",
    "      nn.Linear(),\n",
    "        \n",
    "      nn.ReLU()\n",
    "    )\n",
    "\n",
    "# 20 epochs\n",
    "# mini_batc_size = 100\n",
    "# Loss = MSE\n",
    "# learning_rate = 0.01\n",
    "# momentum = 0.9\n",
    "# TODO: regressor funktion\n",
    "# For layers\n",
    "# \n",
    "# https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html\n",
    "    \n",
    "nn.\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)\n",
    "  \n",
    "    \n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(Net, self).__init__()\n",
    "\n",
    "      # First 2D convolutional layer, taking in 1 input channel (image),\n",
    "      # outputting 32 convolutional features, with a square kernel size of 3\n",
    "      self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "      # Second 2D convolutional layer, taking in the 32 input layers,\n",
    "      # outputting 64 convolutional features, with a square kernel size of 3\n",
    "      self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "\n",
    "      # Designed to ensure that adjacent pixels are either all 0s or all active\n",
    "      # with an input probability\n",
    "      self.dropout1 = nn.Dropout2d(0.25)\n",
    "      self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "      # First fully connected layer\n",
    "      self.fc1 = nn.Linear(9216, 128)\n",
    "      # Second fully connected layer that outputs our 10 labels\n",
    "      self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "my_nn = Net()\n",
    "print(my_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e226ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    def __init__(self, train=True, transform=None):\n",
    "       \n",
    "        #Leaving only image related  columns\n",
    "        feature=df.drop(['age', 'gender', 'race'], axis=1)\n",
    "        #Setting labels\n",
    "        label_age=df['age']\n",
    "        label_gender=df['gender']\n",
    "        label_race=df['race']\n",
    "        #Splitting the data into train and validation set\n",
    "        X_train, X_test, y_age_train, y_age_test, y_gender_train, y_gender_test, y_race_train,\\\n",
    "        y_race_test = train_test_split(feature, label_age, label_gender, label_race, test_size=0.2)\n",
    "        \n",
    "        if train==True:\n",
    "            self.x=X_train\n",
    "            self.age_y=y_age_train\n",
    "            self.gender_y=y_gender_train\n",
    "            self.race_y=y_race_train\n",
    "        else:\n",
    "            self.x=X_test\n",
    "            self.age_y=y_age_test\n",
    "            self.gender_y=y_gender_test\n",
    "            self.race_y=y_race_test            \n",
    "        \n",
    "        #Applying transformation\n",
    "        self.transform=transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image=np.array(self.x.iloc[idx, 0:]).astype(float).reshape(137, 236)\n",
    "        label1=np.array([self.age_y.iloc[idx]]).astype('float')\n",
    "        label2=np.array([self.gender_y.iloc[idx]]).astype('float')\n",
    "        label3=np.array([self.race_y.iloc[idx]]).astype('float')\n",
    "        \n",
    "        sample={'image': np.uint8(image), 'label_age': label1,\\\n",
    "                'label_gender': label2,\\\n",
    "                'label_race': label3}\n",
    "        \n",
    "        #Applying transformation\n",
    "        if self.transform:\n",
    "            sample=self.transform(sample)\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f576d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

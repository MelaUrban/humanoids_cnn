{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1bea3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import color, io\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fcf9057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 36000 files into human database\n",
      "loaded 6000 files into monster database\n",
      "loaded 6000 files into all_humans_annotations database\n",
      "pattern_ii_dataset/humans/dataset/annotations\\female/subject_mesh_0001_anno.json\n",
      "loaded 1000 files into all_monsters_annotations database\n",
      "pattern_ii_dataset/monsters/dataset/annotations\\female/subject_mesh_001_anno.json\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# DATA LOADING\n",
    "##############################################################\n",
    "\n",
    "#########################\n",
    "# HUMANS IMAGES\n",
    "#########################\n",
    "\n",
    "\n",
    "humans_path = 'pattern_ii_dataset/humans/dataset/synthetic_images'\n",
    "\n",
    "all_humans = []\n",
    "    \n",
    "for root, dirs, files in sorted(os.walk(humans_path)):\n",
    "    \n",
    "    for name in files:\n",
    "        all_humans.append(str(root) + \"/\" + str(name))\n",
    "        \n",
    "print(\"loaded %d files into human database\" % len(all_humans))\n",
    "\n",
    "#########################\n",
    "# MONSTERS IMAGES\n",
    "#########################\n",
    "\n",
    "monsters_path = 'pattern_ii_dataset/monsters/dataset/synthetic_images'\n",
    "\n",
    "all_monsters = []\n",
    "    \n",
    "for root, dirs, files in sorted(os.walk(monsters_path)):\n",
    "    \n",
    "    for name in files:\n",
    "        all_monsters.append(str(root) + \"/\" + str(name))\n",
    "        \n",
    "print(\"loaded %d files into monster database\" % len(all_monsters))\n",
    "\n",
    "#########################\n",
    "# HUMANS ANNOTATIONS\n",
    "#########################\n",
    "\n",
    "humans_annotations_path = 'pattern_ii_dataset/humans/dataset/annotations'\n",
    "\n",
    "all_humans_annotations = []\n",
    "    \n",
    "for root, dirs, files in sorted(os.walk(humans_annotations_path)):\n",
    "    \n",
    "    for name in files:\n",
    "        all_humans_annotations.append(str(root) + \"/\" + str(name))\n",
    "        \n",
    "print(\"loaded %d files into all_humans_annotations database\" % len(all_humans_annotations))\n",
    "\n",
    "print(all_humans_annotations[0])\n",
    "\n",
    "#########################\n",
    "# MONSTERS ANNOTATIONS\n",
    "#########################\n",
    "\n",
    "monsters_annotations_path = 'pattern_ii_dataset/monsters/dataset/annotations'\n",
    "\n",
    "all_monsters_annotations = []\n",
    "    \n",
    "for root, dirs, files in sorted(os.walk(monsters_annotations_path)):\n",
    "    \n",
    "    for name in files:\n",
    "        all_monsters_annotations.append(str(root) + \"/\" + str(name))\n",
    "        \n",
    "print(\"loaded %d files into all_monsters_annotations database\" % len(all_monsters_annotations))\n",
    "\n",
    "print(all_monsters_annotations[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a3ac931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern_ii_dataset/humans/dataset/synthetic_images\\200x200\\pose0\\female/subject_mesh_w_rgb_0001.png\n",
      "pattern_ii_dataset/monsters/dataset/synthetic_images\\200x200\\pose0\\female/subject_mesh_w_rgb_001.png\n",
      "pattern_ii_dataset/humans/dataset/synthetic_images\\200x200\\pose0\\female/subject_mesh_w_texture_0001.png\n",
      "pattern_ii_dataset/monsters/dataset/synthetic_images\\200x200\\pose0\\female/subject_mesh_w_texture_001.png\n",
      "pattern_ii_dataset/humans/dataset/synthetic_images\\200x200\\pose0\\female/subject_mesh_0001.png\n",
      "pattern_ii_dataset/monsters/dataset/synthetic_images\\200x200\\pose0\\female/subject_mesh_001.png\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# DATA SORTING\n",
    "##############################################################\n",
    "\n",
    "##############################\n",
    "# MONSTERS\n",
    "##############################\n",
    "\n",
    "monsters_plain_paths = []\n",
    "monsters_rgb_paths = []\n",
    "monsters_texture_paths = []\n",
    "\n",
    "for monster in all_monsters:\n",
    "    if \"rgb\" in monster:\n",
    "        monsters_rgb_paths.append(monster)\n",
    "    elif \"texture\" in monster:\n",
    "        monsters_texture_paths.append(monster)\n",
    "    else:\n",
    "        monsters_plain_paths.append(monster)\n",
    "\n",
    "##############################\n",
    "# HUMANS\n",
    "##############################\n",
    "\n",
    "humans_plain_paths = []\n",
    "humans_rgb_paths = []\n",
    "humans_texture_paths = []\n",
    "\n",
    "for human in all_humans:\n",
    "    if \"rgb\" in human:\n",
    "        humans_rgb_paths.append(human)\n",
    "    elif \"texture\" in human:\n",
    "        humans_texture_paths.append(human)\n",
    "    else:\n",
    "        humans_plain_paths.append(human)\n",
    "        \n",
    "##############################\n",
    "# ANNOTATIONS\n",
    "##############################\n",
    "\n",
    "\n",
    "monsters_annotations_df = pd.DataFrame() \n",
    "\n",
    "for monster_annotation_path in all_monsters_annotations:\n",
    "    \n",
    "    with open (monster_annotation_path, 'r') as file:\n",
    "    \n",
    "        # load from json\n",
    "        monster_annotation = json.load(file)\n",
    "        sub_df = pd.DataFrame.from_dict(monster_annotation['human_dimensions'], orient='index')\n",
    "        sub_df = sub_df.transpose()\n",
    "        \n",
    "        monsters_annotations_df = monsters_annotations_df.append(sub_df, ignore_index=True)\n",
    "\n",
    "\n",
    "humans_annotations_df = pd.DataFrame() \n",
    "\n",
    "for humans_annotation_path in all_humans_annotations:\n",
    "    \n",
    "    with open (humans_annotation_path, 'r') as file:\n",
    "    \n",
    "        # load from json\n",
    "        human_annotation = json.load(file)\n",
    "        sub_df = pd.DataFrame.from_dict(human_annotation['human_dimensions'], orient='index')\n",
    "        sub_df = sub_df.transpose()\n",
    "        \n",
    "        humans_annotations_df = humans_annotations_df.append(sub_df, ignore_index=True)\n",
    "    \n",
    "    \n",
    "#print(humans_annotations_df.head())\n",
    "#print(monsters_annotations_df.head())\n",
    "print(humans_rgb_paths[0])    \n",
    "print(monsters_rgb_paths[0])    \n",
    "print(humans_texture_paths[0])    \n",
    "print(monsters_texture_paths[0])    \n",
    "print(humans_plain_paths[0])    \n",
    "print(monsters_plain_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33ca44f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['chest_circumference', 'height', 'inseam', 'left_arm_length',\n",
      "       'pelvis_circumference', 'right_arm_length', 'shoulder_width',\n",
      "       'waist_circumference'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#print(humans_annotations_df.head())\n",
    "print(monsters_annotations_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b31344f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# CONVERT IMAGES TO GRAYSCALE\n",
    "##############################################################\n",
    "\n",
    "# reduce number of images for testing purposes\n",
    "n_images = 1000000\n",
    "\n",
    "humans_plain = []\n",
    "humans_texture_grey = []\n",
    "humans_rgb_grey = []\n",
    "monsters_plain = []\n",
    "monsters_texture_grey = []\n",
    "monsters_rgb_grey = []\n",
    "\n",
    "##############################\n",
    "# HUMANS PLAIN\n",
    "##############################\n",
    "\n",
    "i = 0\n",
    "\n",
    "for file_path in humans_plain_paths:\n",
    "    human_plain = io.imread(file_path)\n",
    "    \n",
    "    humans_plain.append(human_plain)\n",
    "       \n",
    "    if i == n_images:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "\n",
    "##############################\n",
    "# HUMANS W TEXTURE BACKGROUND\n",
    "##############################\n",
    "\n",
    "i = 0\n",
    "\n",
    "for file_path in humans_texture_paths:\n",
    "    human_texture_grey = color.rgb2gray(io.imread(file_path))\n",
    "    \n",
    "    humans_texture_grey.append(human_texture_grey)\n",
    "       \n",
    "    if i == n_images:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "##############################\n",
    "# HUMANS W RGB BACKGROUND\n",
    "##############################\n",
    "\n",
    "i = 0\n",
    "\n",
    "for file_path in humans_rgb_paths:\n",
    "    human_rgb_grey = color.rgb2gray(io.imread(file_path))\n",
    "    \n",
    "   \n",
    "    humans_rgb_grey.append(human_rgb_grey)\n",
    "       \n",
    "    if i == n_images:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "##############################\n",
    "# MONSTERS PLAIN\n",
    "##############################\n",
    "\n",
    "i = 0\n",
    "\n",
    "for file_path in monsters_plain_paths:\n",
    "    monster_plain = io.imread(file_path)\n",
    "    \n",
    "    monsters_plain.append(monster_plain)\n",
    "       \n",
    "    if i == n_images:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "\n",
    "##############################\n",
    "# MONSTERS W TEXTURE BACKGROUND\n",
    "##############################\n",
    "\n",
    "i = 0\n",
    "\n",
    "for file_path in monsters_texture_paths:\n",
    "    monster_texture_grey = color.rgb2gray(io.imread(file_path))\n",
    "      \n",
    "    monsters_texture_grey.append(monster_texture_grey)\n",
    "       \n",
    "    if i == n_images:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "\n",
    "##############################\n",
    "# MONSTERS W RGB BACKGROUND\n",
    "##############################\n",
    "\n",
    "i = 0\n",
    "\n",
    "for file_path in monsters_rgb_paths:\n",
    "    monster_rgb_grey = color.rgb2gray(io.imread(file_path))\n",
    "    \n",
    "    monsters_rgb_grey.append(monster_rgb_grey)\n",
    "       \n",
    "    if i == n_images:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "\n",
    "    \n",
    "#######################################################################\n",
    "# DISPLAY GREY IMAGES -\n",
    "#######################################################################\n",
    "\n",
    "#cv2.imshow(\"humans_plain\", humans_plain[0])\n",
    "#cv2.imshow(\"humans_rgb_grey\", humans_rgb_grey[0])\n",
    "#cv2.imshow(\"humans_texture_grey\", humans_texture_grey[0])   \n",
    "#cv2.imshow(\"monsters_plain\", monsters_plain[0])     \n",
    "#cv2.imshow(\"monsters_rgb_grey\", monsters_rgb_grey[0])\n",
    "#cv2.imshow(\"monsters_texture_grey\", monsters_texture_grey[5])\n",
    "#cv2.waitKey(0)\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf6a6a0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13824/1686570017.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# tensor of image and annotations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mhumans_plain_full_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhumans_plain_image_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhuman_annotation_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: torch.cat(): Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# CREATE DATA FRAME OF IMAGE VECTORS AND ANNOTATIONS\n",
    "##############################################################\n",
    "\n",
    "##############################\n",
    "# HUMAN PLAIN DATA FRAME\n",
    "##############################\n",
    "\n",
    "\n",
    "# tensor of images w.o. annotations\n",
    "humans_plain_image_tensor = torch.tensor(humans_plain)\n",
    "\n",
    "# tensor of annotations\n",
    "human_annotation_tensor = torch.tensor(humans_annotations_df.values)\n",
    "\n",
    "# copy annotations - pose0 and pose1 have the same annotations\n",
    "human_annotation_tensor = torch.cat((human_annotation_tensor,human_annotation_tensor),0)\n",
    "\n",
    "# tensor of image and annotations\n",
    "humans_plain_full_tensor = torch.cat((humans_plain_image_tensor,human_annotation_tensor),1)\n",
    "\n",
    "\n",
    "##############################\n",
    "# HUMAN RGB DATA FRAME\n",
    "##############################\n",
    "\n",
    "\n",
    "# tensor of images w.o. annotations\n",
    "humans_rgb_image_tensor = torch.tensor(humans_rgb_grey)\n",
    "\n",
    "# tensor of annotations\n",
    "human_annotation_tensor = torch.tensor(humans_annotations_df.values)\n",
    "\n",
    "# copy annotations - pose0 and pose1 have the same annotations\n",
    "human_annotation_tensor = torch.cat((human_annotation_tensor,human_annotation_tensor),0)\n",
    "\n",
    "# tensor of image and annotations\n",
    "humans_rgb_full_tensor = torch.cat((humans_rgb_image_tensor,human_annotation_tensor),1)\n",
    "\n",
    "\n",
    "##############################\n",
    "# HUMAN TEXTURE DATA FRAME\n",
    "##############################\n",
    "\n",
    "\n",
    "# tensor of images w.o. annotations\n",
    "humans_texture_image_tensor = torch.tensor(humans_texture_grey)\n",
    "\n",
    "# tensor of annotations\n",
    "human_annotation_tensor = torch.tensor(humans_annotations_df.values)\n",
    "\n",
    "# copy annotations - pose0 and pose1 have the same annotations\n",
    "human_annotation_tensor = torch.cat((human_annotation_tensor,human_annotation_tensor),0)\n",
    "\n",
    "# tensor of image and annotations\n",
    "humans_texture_full_tensor = torch.cat((humans_texture_image_tensor,human_annotation_tensor),1)\n",
    "\n",
    "\n",
    "##############################\n",
    "# MONSTERS PLAIN DATA FRAME\n",
    "##############################\n",
    "\n",
    "\n",
    "# tensor of images w.o. annotations\n",
    "monsters_plain_image_tensor = torch.tensor(monsters_plain)\n",
    "\n",
    "# tensor of annotations\n",
    "monster_annotation_tensor = torch.tensor(monsters_annotations_df.values)\n",
    "\n",
    "# copy annotations - pose0 and pose1 have the same annotations\n",
    "monster_annotation_tensor = torch.cat((monster_annotation_tensor,monster_annotation_tensor),0)\n",
    "\n",
    "# tensor of image and annotations\n",
    "monsters_plain_full_tensor = torch.cat((monsters_plain_image_tensor,monster_annotation_tensor),1)\n",
    "\n",
    "\n",
    "##############################\n",
    "# MONSTERS RGB DATA FRAME\n",
    "##############################\n",
    "\n",
    "\n",
    "# tensor of images w.o. annotations\n",
    "monster_rgb_image_tensor = torch.tensor(monsters_rgb_grey)\n",
    "\n",
    "# tensor of annotations\n",
    "monster_annotation_tensor = torch.tensor(monsters_annotations_df.values)\n",
    "\n",
    "# copy annotations - pose0 and pose1 have the same annotations\n",
    "monster_annotation_tensor = torch.cat((monster_annotation_tensor,monster_annotation_tensor),0)\n",
    "\n",
    "# tensor of image and annotations\n",
    "monsters_rgb_full_tensor = torch.cat((monster_rgb_image_tensor,monster_annotation_tensor),1)\n",
    "\n",
    "\n",
    "##############################\n",
    "# MONSTERS TEXTURE DATA FRAME\n",
    "##############################\n",
    "\n",
    "\n",
    "# tensor of images w.o. annotations\n",
    "monster_texture_image_tensor = torch.tensor(monsters_texture_grey)\n",
    "\n",
    "# tensor of annotations\n",
    "monster_annotation_tensor = torch.tensor(monsters_annotations_df.values)\n",
    "\n",
    "# copy annotations - pose0 and pose1 have the same annotations\n",
    "monster_annotation_tensor = torch.cat((monster_annotation_tensor,monster_annotation_tensor),0)\n",
    "\n",
    "# tensor of image and annotations\n",
    "monsters_texture_full_tensor = torch.cat((monster_texture_image_tensor,monster_annotation_tensor),1)\n",
    "\n",
    "##############################\n",
    "# HUMAN AND MONSTERS PLAIN\n",
    "##############################\n",
    "\n",
    "humans_monsters_plain_full_tensor = torch.cat((humans_plain_full_tensor, monsters_plain_full_tensor),0)\n",
    "\n",
    "##############################\n",
    "# HUMAN AND MONSTERS RGB\n",
    "##############################\n",
    "\n",
    "humans_monsters_rgb_full_tensor = torch.cat((humans_rgb_full_tensor, monsters_rgb_full_tensor),0)\n",
    "\n",
    "##############################\n",
    "# HUMAN AND MONSTERS TEXTURE\n",
    "##############################\n",
    "\n",
    "humans_monsters_texture_full_tensor = torch.cat((humans_texture_full_tensor, monsters_texture_full_tensor),0)\n",
    "\n",
    "##############################\n",
    "# SAVE TENSORS TO FILES\n",
    "##############################\n",
    "\n",
    "torch.save(humans_plain_full_tensor, 'tensors/humans_plain_full_tensor.pt')\n",
    "print(\"humans_plain_full_tensor.pt saved\")\n",
    "torch.save(humans_rgb_full_tensor, 'tensors/humans_rgb_full_tensor.pt')\n",
    "print(\"humans_rgb_full_tensor.pt saved\")\n",
    "torch.save(humans_texture_full_tensor, 'tensors/humans_texture_full_tensor.pt')\n",
    "print(\"humans_texture_full_tensor.pt saved\")\n",
    "\n",
    "torch.save(monsters_plain_full_tensor, 'tensors/monsters_plain_full_tensor.pt')\n",
    "print(\"monsters_plain_full_tensor.pt saved\")\n",
    "torch.save(monsters_rgb_full_tensor, 'tensors/monsters_rgb_full_tensor.pt')\n",
    "print(\"monsters_rgb_full_tensor.pt saved\")\n",
    "torch.save(monsters_texture_full_tensor, 'tensors/monsters_texture_full_tensor.pt')\n",
    "print(\"monsters_texture_full_tensor.pt saved\")\n",
    "\n",
    "torch.save(humans_monsters_plain_full_tensor, 'tensors/humans_monsters_plain_full_tensor.pt')\n",
    "print(\"humans_monsters_plain_full_tensor.pt saved\")\n",
    "torch.save(humans_monsters_rgb_full_tensor, 'tensors/humans_monsters_rgb_full_tensor.pt')\n",
    "print(\"humans_monsters_rgb_full_tensor.pt saved\")\n",
    "torch.save(humans_monsters_texture_full_tensor, 'tensors/humans_monsters_texture_full_tensor.pt')\n",
    "print(\"humans_monsters_texture_full_tensor.pt saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f74b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index(['chest_circumference', 'height', 'inseam', 'left_arm_length',\n",
    "#       'pelvis_circumference', 'right_arm_length', 'shoulder_width',\n",
    "#       'waist_circumference'],\n",
    "#      dtype='object')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
